{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066212a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakubszafranski/anaconda3/envs/llm_pruning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def wikitext_detokenizer(text):\n",
    "    \"\"\"Detokenize WikiText exactly as lm-eval does\"\"\"\n",
    "    string = text\n",
    "    string = string.replace(\"s '\", \"s'\")\n",
    "    string = re.sub(r\"/' [0-9]/\", r\"/'[0-9]/\", string)\n",
    "    string = string.replace(\" @-@ \", \"-\")\n",
    "    string = string.replace(\" @,@ \", \",\")\n",
    "    string = string.replace(\" @.@ \", \".\")\n",
    "    string = string.replace(\" : \", \": \")\n",
    "    string = string.replace(\" ; \", \"; \")\n",
    "    string = string.replace(\" . \", \". \")\n",
    "    string = string.replace(\" ! \", \"! \")\n",
    "    string = string.replace(\" ? \", \"? \")\n",
    "    string = string.replace(\" , \", \", \")\n",
    "    string = re.sub(r\"\\(\\s*([^\\)]*?)\\s*\\)\", r\"(\\1)\", string)\n",
    "    string = re.sub(r\"\\[\\s*([^\\]]*?)\\s*\\]\", r\"[\\1]\", string)\n",
    "    string = re.sub(r\"{\\s*([^}]*?)\\s*}\", r\"{\\1}\", string)\n",
    "    string = re.sub(r\"\\\"\\s*([^\\\"]*?)\\s*\\\"\", r'\"\\1\"', string)\n",
    "    string = re.sub(r\"'\\s*([^']*?)\\s*'\", r\"'\\1'\", string)\n",
    "    string = string.replace(\"= = = =\", \"====\")\n",
    "    string = string.replace(\"= = =\", \"===\")\n",
    "    string = string.replace(\"= =\", \"==\")\n",
    "    string = string.replace(\" \" + chr(176) + \" \", chr(176))\n",
    "    string = string.replace(\" \\n\", \"\\n\")\n",
    "    string = string.replace(\"\\n \", \"\\n\")\n",
    "    string = string.replace(\" N \", \" 1 \")\n",
    "    string = string.replace(\" 's\", \"'s\")\n",
    "    return string\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset_split='test', max_length=2048):\n",
    "    \"\"\"\n",
    "    Compute WikiText perplexity similar to lm-evaluation-harness.\n",
    "    \n",
    "    This implementation uses DISJOINT (non-overlapping) windows, \n",
    "    which matches the default 'loglikelihood' task in lm-eval.\n",
    "    \n",
    "    Args:\n",
    "        model: HuggingFace model (use model.eval() and model.to(device) before)\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        dataset_split: 'test', 'validation', or 'train'\n",
    "        max_length: Maximum sequence length (model's context window)\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Load WikiText dataset\n",
    "    dataset = load_dataset(\"EleutherAI/wikitext_document_level\", \n",
    "                          \"wikitext-2-raw-v1\", \n",
    "                          split=dataset_split)\n",
    "    \n",
    "    total_loglikelihood = 0.0\n",
    "    total_words = 0\n",
    "    \n",
    "    print(f\"Computing perplexity on {len(dataset)} documents using disjoint windows...\")\n",
    "    \n",
    "    for doc in tqdm(dataset):\n",
    "        # Get original text and detokenize\n",
    "        original_text = doc['page']\n",
    "        detokenized_text = wikitext_detokenizer(original_text)\n",
    "        \n",
    "        # 1. Count words on ORIGINAL text (Your implementation is correct)\n",
    "        words = len(re.split(r\"\\s+\", original_text))\n",
    "        \n",
    "        # 2. Tokenize the DETOKENIZED text (Your implementation is correct)\n",
    "        tokens = tokenizer.encode(detokenized_text, add_special_tokens=False)\n",
    "        \n",
    "        if not tokens:\n",
    "            continue\n",
    "        \n",
    "        # 3. Prepend EOS token (Correct)\n",
    "        tokens = [tokenizer.eos_token_id] + tokens\n",
    "        \n",
    "        # 4. Create DISJOINT windows (This is the corrected part)\n",
    "        doc_loglikelihood = 0.0\n",
    "        for i in range(0, len(tokens), max_length):\n",
    "            window_tokens = tokens[i : i + max_length]\n",
    "            \n",
    "            # Skip windows that are too short to have a label\n",
    "            if len(window_tokens) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Prepare input\n",
    "            input_ids = torch.tensor([window_tokens], device=device)\n",
    "            \n",
    "            # Get model outputs (logits)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            # 5. Compute log-likelihood (Your implementation is correct)\n",
    "            # Shift: predict tokens 1..N given tokens 0..N-1\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            # Compute log probabilities\n",
    "            log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "            \n",
    "            # Gather log probs for actual tokens\n",
    "            token_log_probs = torch.gather(\n",
    "                log_probs, \n",
    "                2, \n",
    "                shift_labels.unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "            \n",
    "            # Sum log probs for this window\n",
    "            doc_loglikelihood += token_log_probs.sum().item()\n",
    "        \n",
    "        total_loglikelihood += doc_loglikelihood\n",
    "        total_words += words\n",
    "    \n",
    "    # 6. Calculate word perplexity (Your implementation is correct)\n",
    "    word_perplexity = np.exp(-total_loglikelihood / total_words)\n",
    "    \n",
    "    return word_perplexity\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load your model\n",
    "#     # model_name = \"gpt2\"  # Replace with your model\n",
    "#     print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "#     model = model.to(\"cuda\")\n",
    "#     model.eval()\n",
    "    \n",
    "#     results = compute_perplexity(model, tokenizer, dataset_split='test')\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"WikiText-2 Perplexity Results\")\n",
    "#     print(f\"Word Perplexity: {results:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188f54cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakubszafranski/anaconda3/envs/llm_pruning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"EleutherAI/wikitext_document_level\", \"wikitext-2-raw-v1\", split=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48dfd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"auxiliary_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e6a09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Storekeeper, the owner of a large hardware store, sells power saws for both personal and commercial use. He often takes old power saws as trade-ins on new ones. The old power saws are then completely disassembled and rebuilt with new bearings by Storekeeper\\'s employees and sold by Storekeeper as \"reconditioned saws.\" Purchaser, the owner and operator of a cabinetmaking shop, informed Storekeeper that he wanted to buy a reconditioned circular saw for use in his cabinetmaking business. However, the blade that was on the saw he picked out had very coarse teeth for cutting rough lumber. Purchaser told Storekeeper that he wanted a saw blade that would cut plywood. Storekeeper exchanged the coarse blade for a new one with finer teeth that would cut plywood smoothly. The new blade was manufactured by Saw-Blade Company, which uses all available techniques to inspect its products for defects. The reconditioned saw had been manufactured by Power Saw Company. The week after the saw was purchased, Employee, who works for Purchaser in Purchaser\\'s cabinetmaking shop, was injured while using the saw. Employee\\'s arm was severely cut. As a result, the cabinetmaking shop was shut down for a week until a replacement for Employee could be foundIf Employee was injured while cutting plywood because the shaft holding the saw blade came loose when a bearing gave way and the shaft and blade flew off the saw, and if Employee asserts a claim based on strict liability in tort against Power Saw Company, Employee will probably',\n",
       " 'subject': '',\n",
       " 'choices': ['recover if the shaft that came loose was a part of the saw when it was new.',\n",
       "  'recover, because Power Saw Company was in the business of manufacturing dangerous machines. ',\n",
       "  'not recover, because Employee was not the buyer of the power saw. ',\n",
       "  'not recover, because the saw had been rebuilt by Storekeepe'],\n",
       " 'answer': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4ea34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0344120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 478.26it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for doc in tqdm(dataset):\n",
    "    original_text = doc['page']\n",
    "    detokenized_text = wikitext_detokenizer(original_text)\n",
    "    \n",
    "    words = len(re.split(r\"\\s+\", original_text))\n",
    "        \n",
    "    pairs.append((original_text, detokenized_text, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "685233ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3934\n"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "print(pairs[page][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04a5e0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3264.1780604133546\n",
      "17708\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "max_words = 0\n",
    "min_words = float('inf')\n",
    "for _, _, words in pairs:\n",
    "    sum += words\n",
    "    if words > max_words:\n",
    "        max_words = words\n",
    "    if words < min_words:\n",
    "        min_words = words\n",
    "print(sum / len(pairs))\n",
    "print(max_words)\n",
    "print(min_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa98e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_pruning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
