# =============================================================================
# LLM Pruning - Configuration
# =============================================================================

random_state: 42

# -----------------------------------------------------------------------------
# Model (LLM to prune)
# -----------------------------------------------------------------------------
model:
  name: meta-llama/Meta-Llama-3.1-8B-Instruct
  dtype: bfloat16

# -----------------------------------------------------------------------------
# Encoder (for state embedding)
# -----------------------------------------------------------------------------
encoder:
  name: answerdotai/ModernBERT-base
  embed_dim: 768

# -----------------------------------------------------------------------------
# Data
# -----------------------------------------------------------------------------
data:
  dataset: wikitext          # wikitext or mmlu
  train_samples: null       # max samples for training (null for all)
  mmlu_subjects: ["high_school_mathematics", "professional_medicine", "high_school_world_history", "high_school_us_history", "professional_law"]    # list of subjects or null for all (only for mmlu)

# -----------------------------------------------------------------------------
# Environment
# -----------------------------------------------------------------------------
environment:
  max_seq_len: 2048          # max tokens for perplexity calculation
  quality_weight: 0.999 
  baseline_perplexity: 6.9
  pruning_type: layer    # cluster64, cluster128, layer

# -----------------------------------------------------------------------------
# SAC Hyperparameters
# -----------------------------------------------------------------------------
# Big training: change buffer: 50k, episodes: 50k, learning_starts: 5k (or multiply by 2)
sac:
  learning_rate: 3.0e-4        
  buffer_size: 20000
  learning_starts: 0        # Collect random steps before training starts           
  batch_size: 128              
  tau: 0.02                   
  gamma: 0.0                   # Immediate reward (Contextual Bandit)
  train_freq: 1
  gradient_steps: 8            # High updates per eval to save data
  
  # Exploration
  use_sde: True                # CRITICAL: Generalized State Dependent Exploration
  sde_sample_freq: -1          # Sample noise once per episode (consistent exploration)
  use_sde_at_warmup: True      # Even during warmup, use structured noise

  # SAC Specifics
  ent_coef: 0.5       
  target_entropy: "auto"    
  
  # Policy network architecture
  pi_layers: [256, 256]        
  qf_layers: [256, 256]        

# -----------------------------------------------------------------------------
# Training
# -----------------------------------------------------------------------------
training:
  total_timesteps: 5000
  log_interval: 100
  save_path: checkpoints/sac_pruning_agent_32
  buffer_save_path: checkpoints/replay_buffer_32
  device: cuda
  verbose: 1

  pretrain_buffer_path: checkpoints/replay_buffer_32
  pretrain_steps: 1500 # set to null to skip pretraining
# -----------------------------------------------------------------------------
# Evaluation
# -----------------------------------------------------------------------------
evaluation:
  deterministic: true        # use deterministic policy (no exploration)





