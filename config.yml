# =============================================================================
# LLM Pruning with PPO - Configuration
# =============================================================================

random_state: 42

# -----------------------------------------------------------------------------
# Grouping (clustering neurons) - now simplified to importance-based pruning
# -----------------------------------------------------------------------------
grouping:
  n_samples: 100
  percentile_threshold: 70
  layers_grouped: 2
  random_state: 42
  
  dim_reduction:
    name: UMAP
    n_components: 40
  
  clustering:
    name: KMeansConstrained
    k: 16
    group_size_deviation: 0.4

# -----------------------------------------------------------------------------
# Neuron Importance (for simplified pruning)
# -----------------------------------------------------------------------------
importance:
  path: neuron_importance_up_proj.json

# -----------------------------------------------------------------------------
# Model (LLM to prune)
# -----------------------------------------------------------------------------
model:
  name: meta-llama/Meta-Llama-3.1-8B-Instruct
  dtype: bfloat16

# -----------------------------------------------------------------------------
# Encoder (for state embedding)
# -----------------------------------------------------------------------------
encoder:
  name: answerdotai/ModernBERT-base
  embed_dim: 768

# -----------------------------------------------------------------------------
# Data
# -----------------------------------------------------------------------------
data:
  dataset: wikitext          # wikitext or mmlu
  train_samples: null       # max samples for training (null for all)
  mmlu_subjects: null    # list of subjects or null for all (only for mmlu)

# -----------------------------------------------------------------------------
# Environment
# -----------------------------------------------------------------------------
environment:
  max_seq_len: 2048          # max tokens for perplexity calculation
  quality_weight: 0.5        # weight for quality (ppl/acc) vs sparsity reward
  baseline_perplexity: 9.57  # optional: if set, use this constant baseline for speed

# -----------------------------------------------------------------------------
# SAC Hyperparameters
# -----------------------------------------------------------------------------
sac:
  learning_rate: 5.0e-5
  buffer_size: 100000
  batch_size: 64
  tau: 0.005
  gamma: 0.0                 # discount factor (0 for contextual bandits)
  train_freq: 1
  gradient_steps: 1
  ent_coef: 'auto'
  target_update_interval: 1
  
  # Policy network architecture
  pi_layers: [512]      # actor hidden layers
  qf_layers: [512]      # critic hidden layers

# -----------------------------------------------------------------------------
# Training
# -----------------------------------------------------------------------------
training:
  total_timesteps: 5000
  log_interval: 50
  save_path: checkpoints/ppo_pruning_agent
  device: cuda             
  verbose: 1

# -----------------------------------------------------------------------------
# Evaluation
# -----------------------------------------------------------------------------
evaluation:
  deterministic: true        # use deterministic policy (no exploration)





