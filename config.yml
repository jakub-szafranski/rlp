# =============================================================================
# LLM Pruning with PPO - Configuration
# =============================================================================

random_state: 42

# -----------------------------------------------------------------------------
# Grouping (clustering neurons)
# -----------------------------------------------------------------------------
grouping:
  n_samples: 100
  percentile_threshold: 70
  layers_grouped: 2
  random_state: 42
  
  dim_reduction:
    name: UMAP
    n_components: 40
  
  clustering:
    name: KMeansConstrained
    k: 16
    group_size_deviation: 0.4

# -----------------------------------------------------------------------------
# Cluster Configuration
# -----------------------------------------------------------------------------
clusters:
  mapping_path: cluster_layer_mapping_up_2l_16c.json
  masks_path: cluster_masks_up_2l_16c.json

# -----------------------------------------------------------------------------
# Model (LLM to prune)
# -----------------------------------------------------------------------------
model:
  name: meta-llama/Meta-Llama-3.1-8B-Instruct
  dtype: bfloat16

# -----------------------------------------------------------------------------
# Encoder (for state embedding)
# -----------------------------------------------------------------------------
encoder:
  name: answerdotai/ModernBERT-base
  embed_dim: 768

# -----------------------------------------------------------------------------
# Data
# -----------------------------------------------------------------------------
data:
  dataset: wikitext          # wikitext or mmlu
  train_samples: 10000       # max samples for training (null for all)
  mmlu_subjects: null    # list of subjects or null for all (only for mmlu)

# -----------------------------------------------------------------------------
# Environment
# -----------------------------------------------------------------------------
environment:
  max_seq_len: 2048          # max tokens for perplexity calculation
  quality_weight: 0.8        # weight for quality (ppl/acc) vs sparsity reward
  baseline_perplexity: 9.57  # optional: if set, use this constant baseline for speed

# -----------------------------------------------------------------------------
# PPO Hyperparameters
# -----------------------------------------------------------------------------
ppo:
  learning_rate: 3.0e-4
  n_steps: 64                # steps per rollout before update
  batch_size: 32
  n_epochs: 4                # PPO epochs per update
  gamma: 0.0                 # discount factor (0 for contextual bandits)
  ent_coef: 0.1              # entropy coefficient - higher for exploration across sparsity levels
  clip_range: 0.2            # PPO clipping parameter
  max_grad_norm: 0.5
  action_bias: -3.0          # initial bias for action logits (negative = favor not pruning)
                             # sigmoid(-2) â‰ˆ 0.12, so initial sparsity ~12%
  
  # Policy network architecture
  pi_layers: [512, 256]      # actor hidden layers
  vf_layers: [512, 256]      # critic hidden layers

# -----------------------------------------------------------------------------
# Training
# -----------------------------------------------------------------------------
training:
  total_timesteps: 10000
  log_interval: 100
  save_path: checkpoints/ppo_pruning_agent
  device: cuda             
  verbose: 1

# -----------------------------------------------------------------------------
# Evaluation
# -----------------------------------------------------------------------------
evaluation:
  deterministic: true        # use deterministic policy (no exploration)





