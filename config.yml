# =============================================================================
# LLM Pruning - Configuration
# =============================================================================

random_state: 42

# -----------------------------------------------------------------------------
# Model (LLM to prune)
# -----------------------------------------------------------------------------
model:
  name: meta-llama/Meta-Llama-3.1-8B-Instruct
  dtype: bfloat16

# -----------------------------------------------------------------------------
# Encoder (for state embedding)
# -----------------------------------------------------------------------------
encoder:
  name: answerdotai/ModernBERT-base
  embed_dim: 768

# -----------------------------------------------------------------------------
# Data
# -----------------------------------------------------------------------------
data:
  dataset: wikitext          # wikitext or mmlu
  train_samples: null       # max samples for training (null for all)
  mmlu_subjects: ["high_school_mathematics", "professional_medicine", "high_school_world_history", "high_school_us_history", "professional_law"]    # list of subjects or null for all (only for mmlu)

# -----------------------------------------------------------------------------
# Environment
# -----------------------------------------------------------------------------
environment:
  max_seq_len: 4096          # max tokens for perplexity calculation
  quality_weight: 0.999       # weight for quality (ppl/acc) vs sparsity reward
  baseline_perplexity: 7.1  # optional: if set, use this constant baseline for speed
  pruning_type: layer    # cluster64, cluster128, layer

# -----------------------------------------------------------------------------
# SAC Hyperparameters
# -----------------------------------------------------------------------------
# Big training: change buffer: 50k, episodes: 50k, learning_starts: 5k (or multiply by 2)
sac:
  learning_rate: 3.0e-4        
  buffer_size: 5000
  learning_starts: 1000        # Collect random steps before training starts           
  batch_size: 512              
  tau: 0.02                   
  gamma: 0.0                   # Immediate reward (Contextual Bandit)
  train_freq: 1
  gradient_steps: 10            # High updates per eval to save data
  
  # Exploration
  use_sde: True                # CRITICAL: Generalized State Dependent Exploration
  sde_sample_freq: -1          # Sample noise once per episode (consistent exploration)
  use_sde_at_warmup: False      # Even during warmup, use structured noise

  # SAC Specifics
  ent_coef: 'auto'             # MUST be 'auto' for automatic tuning
  target_entropy: 'auto'       
  
  # Policy network architecture
  pi_layers: [768, 512]        
  qf_layers: [768, 512]        

# -----------------------------------------------------------------------------
# Training
# -----------------------------------------------------------------------------
training:
  total_timesteps: 5000
  log_interval: 50
  save_path: checkpoints/sac_pruning_agent
  buffer_save_path: checkpoints/replay_buffer
  device: cuda             
  verbose: 1

  pretrain_buffer_path: null # checkpoints/replay_buffer
  pretrain_steps: 5000 # set to null to skip pretraining
# -----------------------------------------------------------------------------
# Evaluation
# -----------------------------------------------------------------------------
evaluation:
  deterministic: true        # use deterministic policy (no exploration)





