# =============================================================================
# LLM Pruning with TD3 - Configuration
# =============================================================================

random_state: 42

# -----------------------------------------------------------------------------
# Grouping (clustering neurons) - now simplified to importance-based pruning
# -----------------------------------------------------------------------------
grouping:
  n_samples: 100
  percentile_threshold: 70
  layers_grouped: 2
  random_state: 42
  
  dim_reduction:
    name: UMAP
    n_components: 40
  
  clustering:
    name: KMeansConstrained
    k: 16
    group_size_deviation: 0.4

# -----------------------------------------------------------------------------
# Neuron Importance (for simplified pruning)
# -----------------------------------------------------------------------------
importance:
  path: neuron_importance_up_proj.json

# -----------------------------------------------------------------------------
# Model (LLM to prune)
# -----------------------------------------------------------------------------
model:
  name: meta-llama/Meta-Llama-3.1-8B-Instruct
  dtype: bfloat16

# -----------------------------------------------------------------------------
# Encoder (for state embedding)
# -----------------------------------------------------------------------------
encoder:
  name: answerdotai/ModernBERT-base
  embed_dim: 768

# -----------------------------------------------------------------------------
# Data
# -----------------------------------------------------------------------------
data:
  dataset: wikitext          # wikitext or mmlu
  train_samples: null       # max samples for training (null for all)
  mmlu_subjects: null    # list of subjects or null for all (only for mmlu)

# -----------------------------------------------------------------------------
# Environment
# -----------------------------------------------------------------------------
environment:
  max_seq_len: 2048          # max tokens for perplexity calculation
  quality_weight: 0.999       # weight for quality (ppl/acc) vs sparsity reward
  baseline_perplexity: 7.25  # optional: if set, use this constant baseline for speed

# -----------------------------------------------------------------------------
# SAC Hyperparameters
# -----------------------------------------------------------------------------
sac:
  learning_rate: 3.0e-4        
  buffer_size: 10000           
  batch_size: 256              
  tau: 0.005                   
  gamma: 0.0                   # Immediate reward (Contextual Bandit)
  train_freq: 2
  gradient_steps: 2            # High updates per eval to save data
  
  # SAC Specifics
  ent_coef: 'auto'             # MUST be 'auto' for automatic tuning
  target_entropy: 'auto'       
  
  # Policy network architecture
  pi_layers: [256, 128]        
  qf_layers: [256, 128]        
  action_bias: 0.05          # initial bias for action mean (fraction to prune per layer)

# -----------------------------------------------------------------------------
# Training
# -----------------------------------------------------------------------------
training:
  total_timesteps: 5000
  log_interval: 50
  save_path: checkpoints/sac_pruning_agent
  buffer_save_path: checkpoints/replay_buffer
  device: cuda             
  verbose: 1

# -----------------------------------------------------------------------------
# Evaluation
# -----------------------------------------------------------------------------
evaluation:
  deterministic: true        # use deterministic policy (no exploration)





